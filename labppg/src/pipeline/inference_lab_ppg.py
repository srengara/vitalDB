#!/usr/bin/env python
"""
Lab PPG Inference Script
=========================
Run inference on lab PPG data without requiring glucose labels.

This script:
1. Loads ppg_windows.csv (generated by generate_lab_training_data.py)
2. Normalizes the PPG data
3. Runs the trained model to predict glucose values
4. Saves predictions to a CSV file

Usage:
    python inference_lab_ppg.py --data_dir ./inference_data --model_path ./model/best_model.pth
    python inference_lab_ppg.py --data_dir ./inference_data --model_path ./model/best_model.pth --distance_multiplier 0.6
"""

import os
import sys
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
from collections import Counter

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from training.resnet34_glucose_predictor import ResidualBlock1D


def build_model_from_checkpoint(state_dict, input_length):
    """
    Auto-detect model architecture from checkpoint state_dict and build matching model.

    Handles two known architectures:
    - Original: conv1 kernel=7, single fc layer
    - Training (Colab): conv1 kernel=3, multi-layer fc head (fc1->fc2->fc_out)
    """
    # Detect conv1 kernel size from weight shape
    conv1_kernel = state_dict['conv1.weight'].shape[2]  # [64, 1, K]

    # Detect FC head type
    has_multi_fc = 'fc1.weight' in state_dict

    class ResNet34_1D_Flex(nn.Module):
        def __init__(self):
            super().__init__()

            # Initial conv - match checkpoint kernel size
            self.conv1 = nn.Conv1d(1, 64, kernel_size=conv1_kernel,
                                   stride=2, padding=conv1_kernel // 2, bias=False)
            self.bn1 = nn.BatchNorm1d(64)
            self.relu = nn.ReLU(inplace=True)
            self.maxpool = nn.MaxPool1d(kernel_size=3, stride=2, padding=1)

            # ResNet layers (same for both architectures)
            self.layer1 = self._make_layer(64, 64, 3, stride=1)
            self.layer2 = self._make_layer(64, 128, 4, stride=2)
            self.layer3 = self._make_layer(128, 256, 6, stride=2)
            self.layer4 = self._make_layer(256, 512, 3, stride=2)

            self.avgpool = nn.AdaptiveAvgPool1d(1)

            # FC head - match checkpoint
            if has_multi_fc:
                self.fc1 = nn.Linear(512, 256)
                self.fc2 = nn.Linear(256, 128)
                self.fc_out = nn.Linear(128, 1)
                self._use_multi_fc = True
            else:
                self.fc = nn.Linear(512, 1)
                self._use_multi_fc = False

        def _make_layer(self, in_ch, out_ch, blocks, stride=1):
            downsample = None
            if stride != 1 or in_ch != out_ch:
                downsample = nn.Sequential(
                    nn.Conv1d(in_ch, out_ch, 1, stride=stride, bias=False),
                    nn.BatchNorm1d(out_ch)
                )
            layers = [ResidualBlock1D(in_ch, out_ch, stride=stride, downsample=downsample)]
            for _ in range(1, blocks):
                layers.append(ResidualBlock1D(out_ch, out_ch))
            return nn.Sequential(*layers)

        def forward(self, x):
            x = self.maxpool(self.relu(self.bn1(self.conv1(x))))
            x = self.layer1(x)
            x = self.layer2(x)
            x = self.layer3(x)
            x = self.layer4(x)
            x = self.avgpool(x)
            x = torch.flatten(x, 1)
            if self._use_multi_fc:
                x = self.relu(self.fc1(x))
                x = self.relu(self.fc2(x))
                x = self.fc_out(x)
            else:
                x = self.fc(x)
            return x

    model = ResNet34_1D_Flex()
    print(f"  Architecture: conv1 kernel={conv1_kernel}, "
          f"FC head={'fc1->fc2->fc_out' if has_multi_fc else 'fc'}")
    return model


def load_ppg_windows(data_dir):
    """Load PPG windows from CSV file"""

    print("=" * 80)
    print("Loading PPG Windows")
    print("=" * 80)

    # Load PPG windows
    ppg_file = os.path.join(data_dir, 'ppg_windows.csv')
    print(f"Loading PPG windows from: {ppg_file}")

    if not os.path.exists(ppg_file):
        raise FileNotFoundError(f"PPG windows file not found: {ppg_file}")

    ppg_df = pd.read_csv(ppg_file)

    # Group by window_index and check lengths
    windows = []
    window_lengths = []
    for window_idx in sorted(ppg_df['window_index'].unique()):
        window_df = ppg_df[ppg_df['window_index'] == window_idx].sort_values('sample_index')
        window = window_df['amplitude'].values
        windows.append(window)
        window_lengths.append(len(window))

    # Find the most common window length
    length_counts = Counter(window_lengths)
    target_length = length_counts.most_common(1)[0][0]

    print(f"Window length statistics:")
    print(f"  Min length: {min(window_lengths)}")
    print(f"  Max length: {max(window_lengths)}")
    print(f"  Target length: {target_length}")
    print(f"  Total windows: {len(windows)}")

    # Pad or truncate windows to target length
    normalized_windows = []
    for window in windows:
        if len(window) == target_length:
            normalized_windows.append(window)
        elif len(window) > target_length:
            # Truncate
            normalized_windows.append(window[:target_length])
        else:
            # Pad with zeros
            padded = np.zeros(target_length)
            padded[:len(window)] = window
            normalized_windows.append(padded)

    ppg_data = np.array(normalized_windows)
    print(f"[OK] Loaded {len(ppg_data)} PPG windows")
    print(f"  Shape: {ppg_data.shape}")

    return ppg_data


def normalize_ppg(ppg_data, ppg_mean=None, ppg_std=None, use_global=True):
    """
    Normalize PPG data.

    Args:
        ppg_data: PPG windows array
        ppg_mean: Global PPG mean from training (if use_global=True)
        ppg_std: Global PPG std from training (if use_global=True)
        use_global: If True, use global normalization (same as training)
                   If False, use per-window normalization
    """
    if use_global and ppg_mean is not None and ppg_std is not None:
        # Global normalization (matches training)
        normalized_ppg = (ppg_data - ppg_mean) / ppg_std
    else:
        # Per-window normalization (fallback)
        ppg_mean_local = np.mean(ppg_data, axis=1, keepdims=True)
        ppg_std_local = np.std(ppg_data, axis=1, keepdims=True)
        ppg_std_local[ppg_std_local == 0] = 1.0
        normalized_ppg = (ppg_data - ppg_mean_local) / ppg_std_local
    return normalized_ppg


def run_inference(model_path, data_dir, output_dir=None, glucose_mean=None, glucose_std=None, distance_multiplier=0.8):
    """
    Run inference on PPG data to predict glucose values.

    Args:
        model_path: Path to trained model checkpoint
        data_dir: Directory containing ppg_windows.csv
        output_dir: Output directory for predictions (default: data_dir/predictions)
        glucose_mean: Glucose mean used during training (optional, overrides checkpoint)
        glucose_std: Glucose std used during training (optional, overrides checkpoint)
        distance_multiplier: Peak distance multiplier for window extraction (default: 0.8)
    """
    print("\n" + "=" * 80)
    print("LAB PPG GLUCOSE INFERENCE")
    print("=" * 80)
    print(f"Model: {model_path}")
    print(f"Data Directory: {data_dir}")
    print(f"Distance Multiplier: {distance_multiplier}")

    # Load model first to get normalization parameters
    print("\n" + "=" * 80)
    print("Loading Model")
    print("=" * 80)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Device: {device}")

    checkpoint = torch.load(model_path, map_location=device, weights_only=False)

    print(f"[OK] Model loaded successfully")
    print(f"  Epoch: {checkpoint['epoch']}")

    # Get normalization parameters from checkpoint
    norm = checkpoint.get('normalization', {})
    if isinstance(norm, dict):
        default_glucose_mean = norm.get('glucose_mean', checkpoint.get('glucose_mean', 0.0))
        default_glucose_std = norm.get('glucose_std', checkpoint.get('glucose_std', 1.0))
        ppg_norm_type = norm.get('ppg_normalization', 'per_window')
        ppg_global_mean = norm.get('ppg_mean', None)
        ppg_global_std = norm.get('ppg_std', None)
    else:
        default_glucose_mean = checkpoint.get('glucose_mean', 0.0)
        default_glucose_std = checkpoint.get('glucose_std', 1.0)
        ppg_norm_type = 'per_window'
        ppg_global_mean = None
        ppg_global_std = None

    if glucose_mean is None:
        glucose_mean = default_glucose_mean
    if glucose_std is None:
        glucose_std = default_glucose_std

    print(f"\nNormalization parameters from checkpoint:")
    print(f"  Glucose mean: {glucose_mean:.2f} mg/dL")
    print(f"  Glucose std: {glucose_std:.2f} mg/dL")
    print(f"  PPG normalization type: {ppg_norm_type}")
    if ppg_global_mean is not None:
        print(f"  PPG global mean: {ppg_global_mean:.4f}")
        print(f"  PPG global std: {ppg_global_std:.4f}")

    # Load PPG data
    ppg_data = load_ppg_windows(data_dir)
    input_length = ppg_data.shape[1]

    # Normalize PPG data using SAME method as training
    print("\n" + "=" * 80)
    print("Normalizing PPG Data")
    print("=" * 80)

    use_global = (ppg_norm_type == 'global' and ppg_global_mean is not None)
    if use_global:
        print(f"  Using GLOBAL normalization (same as training)")
        ppg_normalized = normalize_ppg(ppg_data, ppg_global_mean, ppg_global_std, use_global=True)
    else:
        print(f"  Using PER-WINDOW normalization (fallback)")
        ppg_normalized = normalize_ppg(ppg_data, use_global=False)

    print(f"[OK] PPG data normalized")
    print(f"  Normalized range: {ppg_normalized.min():.4f} - {ppg_normalized.max():.4f}")

    # Build and load model
    model = build_model_from_checkpoint(checkpoint['model_state_dict'], input_length)
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
    model.eval()

    print(f"\nGlucose denormalization parameters:")
    print(f"  Mean: {glucose_mean:.2f} mg/dL")
    print(f"  Std: {glucose_std:.2f} mg/dL")

    if glucose_mean == 0.0 and glucose_std == 1.0:
        print("  [WARNING] Using default normalization parameters (0.0, 1.0)")
        print("  [WARNING] Predictions may not be in correct mg/dL scale")
        print("  [HINT] Use --glucose_mean and --glucose_std arguments to specify training normalization")

    # Run predictions
    print("\n" + "=" * 80)
    print("Running Predictions")
    print("=" * 80)

    all_predictions_normalized = []

    batch_size = 32
    num_samples = len(ppg_normalized)

    with torch.no_grad():
        for i in range(0, num_samples, batch_size):
            # Get batch (handles last batch correctly)
            end_idx = min(i + batch_size, num_samples)
            batch_ppg = ppg_normalized[i:end_idx]

            # Convert to tensor and add channel dimension
            batch_tensor = torch.tensor(batch_ppg, dtype=torch.float32).unsqueeze(1).to(device)

            # Predict (outputs normalized values)
            predictions = model(batch_tensor)

            # Append predictions
            batch_predictions = predictions.cpu().numpy().flatten()
            all_predictions_normalized.extend(batch_predictions)

    predictions_normalized = np.array(all_predictions_normalized)

    print(f"[OK] Generated {len(predictions_normalized)} predictions")
    print(f"  Normalized predictions range: {predictions_normalized.min():.4f} - {predictions_normalized.max():.4f}")

    # Denormalize predictions
    print("\n" + "=" * 80)
    print("Denormalizing Predictions")
    print("=" * 80)

    predictions_mgdl = predictions_normalized * glucose_std + glucose_mean

    print(f"Predicted glucose values:")
    print(f"  Range: {predictions_mgdl.min():.2f} - {predictions_mgdl.max():.2f} mg/dL")
    print(f"  Mean: {predictions_mgdl.mean():.2f} mg/dL")
    print(f"  Std: {predictions_mgdl.std():.2f} mg/dL")
    print(f"  Median: {np.median(predictions_mgdl):.2f} mg/dL")

    # Clinical interpretation
    print("\nClinical Interpretation (based on mean):")
    mean_glucose = predictions_mgdl.mean()
    if mean_glucose < 70:
        print(f"  [LOW] Hypoglycemia: {mean_glucose:.2f} mg/dL < 70 mg/dL")
    elif mean_glucose <= 100:
        print(f"  [NORMAL] Normal fasting: 70 <= {mean_glucose:.2f} <= 100 mg/dL")
    elif mean_glucose <= 125:
        print(f"  [ELEVATED] Prediabetes: 100 < {mean_glucose:.2f} <= 125 mg/dL")
    else:
        print(f"  [HIGH] Diabetes range: {mean_glucose:.2f} > 125 mg/dL")

    # Save results
    if output_dir is None:
        output_dir = os.path.join(data_dir, 'predictions')

    os.makedirs(output_dir, exist_ok=True)

    results_df = pd.DataFrame({
        'window_index': range(len(predictions_mgdl)),
        'predicted_glucose_mg_dl': predictions_mgdl
    })

    output_file = os.path.join(output_dir, 'glucose_predictions.csv')
    results_df.to_csv(output_file, index=False)
    print(f"\n[OK] Predictions saved to: {output_file}")

    # Show sample predictions
    print("\nSample Predictions (first 20 windows):")
    print(f"{'Window':<10} {'Predicted Glucose (mg/dL)':<25}")
    print("-" * 35)
    for i in range(min(20, len(predictions_mgdl))):
        print(f"{i:<10} {predictions_mgdl[i]:<25.2f}")

    if len(predictions_mgdl) > 20:
        print(f"... and {len(predictions_mgdl) - 20} more windows")

    print("\n" + "=" * 80)
    print("INFERENCE COMPLETED")
    print("=" * 80)
    print(f"\nSummary:")
    print(f"  Total windows processed: {len(predictions_mgdl)}")
    print(f"  Average predicted glucose: {predictions_mgdl.mean():.2f} mg/dL")
    print(f"  Predictions saved to: {output_file}")

    return predictions_mgdl


if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(
        description='Run glucose inference on lab PPG data',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Basic usage
  python inference_lab_ppg.py --data_dir ./inference_data --model_path ./model/best_model.pth

  # Specify output directory
  python inference_lab_ppg.py --data_dir ./inference_data --model_path ./model/best_model.pth --output ./results

  # Custom distance multiplier for peak detection
  python inference_lab_ppg.py --data_dir ./inference_data --model_path ./model/best_model.pth --distance_multiplier 0.6
        """
    )

    parser.add_argument('--model_path', type=str, required=True,
                       help='Path to trained model checkpoint (.pth file)')
    parser.add_argument('--data_dir', type=str, required=True,
                       help='Directory containing ppg_windows.csv')
    parser.add_argument('--output', type=str, default=None,
                       help='Output directory for predictions (default: data_dir/predictions)')
    parser.add_argument('--glucose_mean', type=float, default=None,
                       help='Glucose mean used during training (overrides checkpoint value)')
    parser.add_argument('--glucose_std', type=float, default=None,
                       help='Glucose std used during training (overrides checkpoint value)')
    parser.add_argument('--distance_multiplier', type=float, default=0.8,
                       help='Peak distance multiplier for window extraction (default: 0.8)')

    args = parser.parse_args()

    # Validate inputs
    if not os.path.exists(args.model_path):
        print(f"[ERROR] Model file not found: {args.model_path}")
        sys.exit(1)

    if not os.path.exists(args.data_dir):
        print(f"[ERROR] Data directory not found: {args.data_dir}")
        sys.exit(1)

    ppg_file = os.path.join(args.data_dir, 'ppg_windows.csv')
    if not os.path.exists(ppg_file):
        print(f"[ERROR] ppg_windows.csv not found in: {args.data_dir}")
        print(f"Expected file: {ppg_file}")
        sys.exit(1)

    # Run inference
    try:
        run_inference(args.model_path, args.data_dir, args.output,
                     args.glucose_mean, args.glucose_std, args.distance_multiplier)
    except Exception as e:
        print(f"\n[ERROR] Inference failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
